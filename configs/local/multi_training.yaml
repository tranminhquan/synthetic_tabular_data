source_domain_dir: data/sdgym_filtered # directory that store all sub-dirs, each one has a csv file and a metadata.json (if any)
target_domain_dir: # if empty, automatically apply kfold (1-vs-rest) to get target domain
n_folds: 3 # number of fold to split if target directory is not indicated
preprocessing:
  clean_data: True
  clean_exclude_columns: [] # exclude chosen columns to apply cleaning, if empty, all columns are considered
  min_freq_threshold: 0.
  percentage_to_remove: 0.8
transform:
  max_cols: 100 # max column when apply transformation. If after transforming, the tabular cols < max_cols, add padding, else crop tabular cols to get max_cols
  signature_settings:
    column_name_embedding: False # whether to apply BERT to encode column name or not
    quantile: False # whether to apply quantile as a signature feature or not
  categorical_columns:  # if None, auto detect categorical columns in df
  categorical_settings:
    normalize: True # If True, apply FrequencyEncoding, else only encode by counting
  numerical_settings:
    max_gaussian_components: 3
    gaussian_weight_threshold: 5.e-3
model:
  encoder_hiddens: # each row below represents a layer, and the number represents the dimension of that layer
    - 128
    - 128
  decoder_hiddens:  # each row below represents a layer, and the number represents the dimension of that layer
    - 128
    - 128
  embedding_dim: 128 # embedding dim of the output of the encoder
training:
  batch_size: 128
  epochs: 50
  shuffle_training: True
  lr: 1.e-3
  l2norm: 1.e-5
  use_gpu: True
  recloss_factor: 1.
  optimize_signature_features: False
  save_dir: results/multitrain
  save_checkpoint: False # if True, the model is saved for each training iteration for each source domain dataset
  model_prefix: model_final # final model after training all source domain datasets
finetuning:
  finetune_size: 0.2 # percentage of amount of data for finetuning
